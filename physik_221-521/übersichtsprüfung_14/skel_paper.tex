\documentclass[a4paper,10pt]{article}

%{{{ packages
\usepackage{blindtext}
\usepackage{lipsum}
\usepackage[ngerman]{babel}
\usepackage[tiny]{titlesec}
\usepackage{index}
\usepackage[onehalfspacing]{setspace}
\usepackage{fullpage}
\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{authblk}
\usepackage{hyperref}
\hypersetup{colorlinks=true,allcolors=blue}

%\usepackage{tabulary}
%\usepackage{tabularx}
\usepackage{booktabs}%toprule,midrule,bottomrule
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tikz}
\usepackage[european,siunitx]{circuitikz}
\usepackage{graphicx}
\usepackage{svg} %\includesvg{}
\usepackage{import} % can import files from other directories
\usepackage{longtable}
\usepackage{pgfplots}
\usepackage{gnuplottex}
\usepackage{wrapfig}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{tensor} %for indices
\usepackage{dsfont} % double stroke font
\usepackage{cancel} % cancel in frac
\usepackage{bm} % bold math font (if error is produced use \bm{{}})
\usepackage{mathtools}
\usepackage[ISO]{diffcoeff}
\usepackage[locale=DE]{siunitx}
\usepackage[official]{eurosym}
%\usepackage{mathrsfs} % calligraphy font
\usepackage{physics}
\usepackage[a]{esvect}
\usepackage{bigints}
%\usepackage[frak=esstix]{mathalpha} % disable if LaTeX uses too many alphabets
\usepackage{siunitx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{listings}

\usepackage{babel}
\usepackage{hyphsubst}
\usepackage{caption}
\usepackage{xr} % crossreferencing between documents \externaldocument{}
\usepackage{enumitem} % for enumerate environment
\usepackage{lineno}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{color}

\allowdisplaybreaks % allows equations to be broken (e.g. by multicols)

\usetikzlibrary{arrows}
\pgfplotsset{compat=1.15}

\newcommand{\td}{\,\text{d}}
\newcommand{\RN}[1]{\uppercase\expandafter{\romannumeral#1}}
\newcommand{\zz}{\mathrm{Z\kern-.3em\raise-0.5ex\hbox{Z} }}
\newcommand{\id}{\mathds{1}}

\newcommand\inlineeqno{\stepcounter{equation}\ {(\theequation)}}
\newcommand\inlineeqnoa{(\theequation.\text{a})}
\newcommand\inlineeqnob{(\theequation.\text{b})}
\newcommand\inlineeqnoc{(\theequation.\text{c})}

\newcommand\inlineeqnowo{\stepcounter{equation}\ {(\theequation)}}
\newcommand\inlineeqnowoa{\theequation.\text{a}}
\newcommand\inlineeqnowob{\theequation.\text{b}}
\newcommand\inlineeqnowoc{\theequation.\text{c}}

\renewcommand{\refname}{Source}
\renewcommand{\sfdefault}{phv}

\iffalse\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}\fi % for multicols figures

%\pagestyle{fancy}

\sloppy

\numberwithin{equation}{section}

\titleformat{\subsection}{}{\thesubsection}{1em}{\itshape}
\titleformat{\subsubsection}{}{\thesubsubsection}{1em}{\itshape}

%}}}

\begin{document}

%{{{ Titelseite

\begin{titlepage}
  \title{Statistical Mechanics}
  \author{Jonas Wortmann\thanks{s02jwort@uni-bonn.de}}
  \affil{Rheinische Friedrich--Wilhelms--Universit√§t Bonn}
\end{titlepage}

\maketitle
\pagenumbering{arabic}

\renewcommand\abstractname{Abstract}
\abstract{\noindent This work is a short summary on the derivation of the methods of statistical mechanics as well as its application to various systems whether they be discrete or continuous.
The focus is on ensemble theory.}

%}}}

%\clearpage

%{{{ Inhaltsverzeichnis

\fancyhead[R]{\leftmark}
%\fancyhead[R]{\leftmark\\\rightmark}
\fancyhead[L]{\thepage}
\fancyfoot[C]{}

\renewcommand\contentsname{Contents}
\tiny
\tableofcontents
\normalsize

%}}}

%\clearpage

\vspace{1em}
\hrule

\begin{multicols}{2}

%{{{

% {{{ Ensemble Theory
\section{Ensemble Theory}
Imagine one would like to study the microscopic behaviour of a many particle system.
It is impossible to solve the equations of motions for every particle analytically, thus one has to rely on probability theory and statistical mechanics.
For a system one can therefore measure macroscopic quantities e.g.\ pressure, volume or temperature.
A set of these quantities with certain values makes out the macrostate of the system.
On the microscopic scale (i.e.\ positions, momenta) however, it is possible to realize this macrostate in many ways.
A state which realizes a certain macrostate is called a microstate $\left(=:\mathfrak{m}\right)$.
There is often more than one microstate to correspond to / realize a certain macrostate.
For a many particle system made up of $N$ particles, a microstate is a single point in a $6N$ dimensional phase space with $3N$ positions and $3N$ momenta.
The time evolution is a single curve through this phase space, thus a series of different microstates (which each have to realize the same macrostate).
One can imagine a gas inside a box: The positions of all gas particles will change over time however the volume will always stay the same.

To study the macrostate from a microstate one can take a time average average of all microstates which have been on the trajectory.
Instead one could also make use of the ergodic theorem and average a multitude of identically prepared system to achieve the same average.
The set of all identically prepared systems are then called an ensemble.
These systems are, in general, only imaginary copies of a single system and they are considered in the calculation as possible microstates for a system (i.e.\ they don't have to be considered as a separate system for which there are neccessary calculation to do).
The use of this theory is not apparent when one considers a precisely known macroscopic system i.e.\ all macrostates are fixed.
However, if one only fixes a certain amount of macrostates and leaves e.g.\ the energy to be arbitrary, then the restriction on the phase space trajectory is lifted in that dimension and the microstates allow for an arbitrary range of energies.
Then one can utilise the methods of the theory to calculate the average energy of such a system by summing over all energies (which are realized in a certain microstate) and weighing them with a probability function.
%}}}

%{{{ Probability Distribution and Partition Sum
\section{Probability Distribution and Partition Sum}

%{{{ Derivation
\subsection{Derivation}
Imagine a certain set of available data or information about a probability distribution:
To derive the underlying distribution function, the best model is the one, which only assumes the least amount of knowledge about it.
Thus, to find a suitable probability distribution which models the microstates for a given macrostate one maximizes the entropy (principle of maximal entropy), since the entropy encodes the lack of information on a system (or, in this case, is proportional to the amount of available microstates).

The entropy is given by
\begin{align} 
  \boxed{S = -k_B\sum_{i}^{}p_i \ln p_i\equiv -k_B\text{tr}\left(\rho \ln \rho \right)}
,\end{align} 
with $p_i$ being the probability that the $i^{\text{th}}$ microstate is realized and $\rho $ being the density matrix.
The only constraints are that the probability distribution may be normalized and that it is used to calculate the average value of any observable $A^l$ 
\begin{align} 
  f_0 &= \sum_{i}^{}p_i-1=0 & f_l &= \sum_{i}^{}p_iA^l_i-\left\langle A^l_i\right\rangle =0
,\end{align} 
while the sum of $f_l$ ranges over all $i$ microstates with their corresponding value of $A_i^l$.
The Lagrangian is
\begin{align} 
  \mathcal{L} &= S - k_B\lambda _0f_0 - k_B\sum_{l}^{}\lambda _lf_l\\
              &\begin{multlined}
              = -k_B\sum_{i}^{}p_i\ln p_i - k_B\lambda _0\left(\sum_{i}^{}p_i-1\right) \\- k_B\sum_{l}^{}\lambda _l\left(\sum_{i}^{}p_iA_i^l-\left\langle A^l\right\rangle \right)
              \end{multlined}
.\end{align} 
Now taking the variation w.r.t.\ to all $p_i$ equal to zero
\begin{multline}
  \delta _{p_i}\mathcal{L} = k_B\left(-\sum_{i}^{}\ln p_i - \sum_{i}^{}1 - \lambda _0\sum_{i}^{}1 \right.\\ -\left.\sum_{l}^{}\lambda _l \sum_{i}^{}A_i^l\right)\delta p_i=0
.\end{multline}
Exchanging $\sum_{l}^{}$ and $\sum_{i}^{}$ in the last term and taking $\sum_{i}^{}$ out of the parentheses yields
\begin{align} 
  -\ln p_i - 1 - \lambda _0 - \sum_{l}^{}\lambda _lA^l_i &= 0
,\end{align} 
because $\delta _{p_i}\mathcal{L}=0\,\forall \delta p_i$ independently.
\begin{align} 
  p_i &= \text{e}^{-(1 + \lambda _0) - \sum_{l}^{}\lambda _lA_i^l} = \text{e}^{-(1+\lambda _0)}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}
.\end{align} 
The normalisation multiplier yields
\begin{align} 
  \sum_{i}^{}p_i &= \sum_{i}^{}\text{e}^{-(1+\lambda _0)}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}\\
                 &= \text{e}^{-(1+\lambda _0)}\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}\\
                 &= 1\\
  \Leftrightarrow \text{e}^{-(1+\lambda _0)} &= \dfrac{1}{\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}=:\dfrac{1}{Z}
.\end{align} 
Thereby
\begin{align} 
  \boxed{p_i=\dfrac{\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}{\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}=:\dfrac{\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}{Z}}
.\end{align} 
The other Lagrange multiplier yields
\begin{align} 
  \sum_{i}^{}p_iA_i^l &= \sum_{i}^{}\dfrac{\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}{\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}A_i^l\\
                      &= \dfrac{1}{Z}\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}A_i^l\\
                      &= -\dfrac{1}{Z}\diffp*[]{\sum_{i}^{}\text{e}^{-\sum_{l}^{}\lambda _lA_i^l}}{\lambda _l}\\
                      &= \boxed{-\diffp[]{\ln Z}{\lambda _l} = \left\langle A^l\right\rangle }
.\end{align} 
The relation $-\diffp[]{\ln Z}{\lambda _l} = \left\langle A^l\right\rangle $ is used to determine the Lagrange multiplier.
It is also useful to know
\begin{align} 
  \diffp[2]{\ln Z}{\lambda _l} &= \left\langle A^l\right\rangle ^2-\left\langle {A^l}^2\right\rangle \\
  \diffp[]{\ln Z}{\lambda _k,\lambda _l} &= \left\langle A^lA^k\right\rangle -\left\langle A^l\right\rangle \left\langle A^k\right\rangle 
.\end{align}
Recall the general formula for entropy
\begin{align} 
  S &= -k_B\sum_{i}^{}p_i\ln p_i\\
    &= -k_B\left\langle \ln p_i\right\rangle \\
    &= k_B\left\langle \ln Z\right\rangle -k_B\left\langle \ln \text{e}^{-\sum_{l}^{}\lambda _lA^l_i}\right\rangle \\
    &= k_B\ln Z + k_B\left\langle \sum_{l}^{}\lambda _lA_i^l\right\rangle \\
    &= k_B\ln Z + k_B\sum_{l}^{}\lambda _l\left\langle A_i^l\right\rangle \\
\Aboxed{S &= k_B\ln Z - k_B\sum_{l}^{}\lambda _l\diffp[]{\ln Z}{\lambda _l}}
,\end{align} 
with $\left\langle A_i^l\right\rangle =\left\langle A^l\right\rangle $ since !TODO!.

Consider the derivative of $S$ w.r.t.\ $\left\langle A^l\right\rangle $ and choose $A=E$ with $\lambda =\beta $ as the energy
\begin{align} 
  \diffp[]{S}{\left\langle E\right\rangle } &= k_B\beta =\dfrac{1}{T}\Leftrightarrow \boxed{\beta =\dfrac{1}{k_BT}}
\end{align} 
by comparison with the thermodynamic potential of the entropy and its derivatives\footnote{The assumption $U=\left\langle E\right\rangle $ is phenomenological and not rigorous; another way to show this would be by calculating the total differential $\td S$.}.
If one were to set $A=N$ and $\lambda =-\alpha $ as the particle number
\begin{align} 
  \diffp[]{S}{\left\langle N\right\rangle }=-k_B\alpha =-\dfrac{\mu }{T}\Leftrightarrow \boxed{\alpha =\dfrac{\mu }{k_BT}}
.\end{align} 

%}}}

%{{{ Microcanonical Ensemble
\subsection{Microcanonical Ensemble}
The ensemble is specified to have a fixed pressure, volume, energy and particle number.
Because all observables are fixed, only the normalisation constraint $\sum_{i}^{}p_i-1=0$ has to be fulfilled.
Thus
\begin{align} 
  p_i=\dfrac{1}{Z}=\dfrac{1}{\Omega }
,\end{align} 
with $\Omega =\sum_{\mathfrak{m}}^{}1$ the amount of available microstates or the dimension of the Hilbert space.
The entropy is
\begin{align} 
  S &= -k_B\sum_{i}^{}p_i\ln p_i=-k_B\left\langle \ln p_i\right\rangle =k_B\ln \Omega 
,\end{align} 
which is also a suitable thermodynamic potential.
%}}}

%{{{ Canonical Ensemble
\subsection{Canonical Ensemble}
The ensemble is specified to have a fixed pressure, volume, temperature and particle number resulting in an unknown energy value, whose mean has to be calculated.
The probability that a certain microstate $\mathfrak{m}$ realizes a macrostate whose energy is $E_\mathfrak{m}$ is given by
\begin{align} 
  p_\mathfrak{m} &= \dfrac{\text{e}^{-\beta E_\mathfrak{m} }}{\sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_\mathfrak{m} }} & \left\langle E\right\rangle &=\sum_{i}^{}p_iE_i
,\end{align} 
which in turn results in the average energy value $\left\langle E\right\rangle $.
The distribution is normalized by the partition function (the sum can also the form of an integral in the classical continuous sense)
\begin{align} 
  Z &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_\mathfrak{m}}
\end{align} 
which accounts for all possible values the energy $E$ can take, i.e.\ all possible microstates which have to realize the macrostate $\left(P,V,T\right)$ but are free to take whatever value in $E$.

The entropy is given by
\begin{align} 
  S &= -k_B\sum_{i}^{}p_i\ln p_i\\
    &= -k_B\sum_{i}^{}p_i\left(-\beta E_i-\ln Z\right)\\
    &= k_B\beta \left\langle E\right\rangle +k_B\ln Z\\
    &= \dfrac{U}{T}+k_B\ln Z \label{eq:entropy_partition_sum}
.\end{align} 
To calculate thermodynamic properties the suitable potential is the free energy 
\begin{align} 
  F=U-TS \Leftrightarrow S=\dfrac{U}{T}-\dfrac{F}{T}
.\end{align} 
By using (\ref{eq:entropy_partition_sum})
\begin{align} 
  \boxed{F=-k_BT\ln Z}
\end{align} 
and with
\begin{align} 
  \td F &= -S\td T-p\td V+\mu \td N
,\end{align} 
thermodynamic properties can be calculated from the microscopic partition sum.
%}}}

%{{{ Example: Dipoles in a Magnetic Field
\subsubsection{Example: Dipoles in a Magnetic Field}
As an example consider a system of $N$ dipoles (non-interacting, localized, freely orientable) in a magnetic field.
\begin{align} 
          Z &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_\mathfrak{m}}\\
            &= \sum_{ \left\{\theta \right\}}^{}\text{e}^{\sum_{i}^{}h\cos \theta _i}\\
            &= \sum_{ \left\{\theta \right\}}^{}\prod_{i}^{}\text{e}^{h\cos \theta _i}\\
            &= \sum_{\theta _1}^{}\sum_{\theta _2}^{}\hdots \sum_{\theta _N}^{}\text{e}^{h\cos \theta _1}\text{e}^{h\cos \theta _2}\hdots \text{e}^{h\cos \theta _N}\\
            &= \sum_{\theta _1}^{}\text{e}^{h\cos \theta _1}\sum_{\theta _2}^{}\text{e}^{h\cos \theta _2}\hdots \sum_{\theta _N}^{}\text{e}^{h\cos \theta _N} \label{eq:factorization}\\
            &= \prod_{i}^{}\sum_{\theta _i}^{}\text{e}^{h\cos \theta _i}\\
            &= \left(\sum_{\theta }^{}\text{e}^{h\cos \theta }\right)^N\\
            &= \left(Z_1\right)^N
.\end{align} 
The factorization of $Z$ in (\ref{eq:factorization}) is only possible because the dipoles are non-interacting; there are no $\theta _{ij}$ factors which would make it impossible to split the product that way.
A microstate $\mathfrak{m}$ of this system is able to realize a certain macrostate $\left(P,V,T\right)$ with arbitrary energy $E_\mathfrak{m}$.
The energy of the system or that a microstate $\mathfrak{m}$ can take is $E_\mathfrak{m}=\sum_{i}^{N}\varepsilon _i=h\sum_{i}^{}\cos \theta _i$ the sum of all individual dipole energies, which is a proportionality factor times the cosine of the angle relative to the magnetic field (thus the degree of freedom is the angle relative to the applied field).
Since system consists of $N$ dipoles and it is possible for every dipole to have every possible rotation for every possible rotation for every $N-1$ other dipole, the available microstates are
\begin{align} 
  \sum_{\mathfrak{m}}^{}=\sum_{\theta _1}^{}\sum_{\theta _2}^{}\hdots \sum_{\theta _N}^{}
,\end{align} 
with $\sum_{\theta _i}^{}$ summing over all possible values for the angle of the $i^{\text{th}}$ dipole.
This is also written as $\sum_{\mathfrak{m}}^{}=\sum_{ \left\{\theta \right\}}^{}$ (or sometimes as $=\sum_{\theta }^{}$ which might be misleading in some cases).
To analyse further one can treat this in the classical sense and have the values of $\theta $ be continuous (if they were dicrete one would leave $Z_1$ as it is)
\begin{align} 
  Z_1 &= \int_{0}^{2\pi }\td \phi \int_{0}^{\pi }\td \theta \text{e}^{h\cos \theta }=4\pi \dfrac{\sinh h}{h}
.\end{align}
The $\phi $ angle lies in the plane perpendicular to the magnetic field and has to be accounted for since the dipole does not need to lie in a plane.

If one were to use the energies $\varepsilon _i$ as a label instead the sum would look like
\begin{align} 
  \sum_{\mathfrak{m}}^{}=\sum_{\varepsilon _1}^{}\sum_{\varepsilon _2}^{}\hdots \sum_{\varepsilon _N}^{}
,\end{align} 
with $E_\mathfrak{m}=\sum_{i}^{N}\varepsilon _i$ as above.
%}}}

%{{{ Example: Harmonic Oscillator
\subsubsection{Example: Harmonic Oscillator}
This could also be used for a non-interacting set of harmonic oscillators in which the label for the energy would be $n$ the harmonic quantum number
\begin{align} 
  \sum_{\mathfrak{m}}^{}=\sum_{n_1}^{}\sum_{n_2}^{}\hdots \sum_{n_N}^{}
,\end{align} 
with $E_\mathfrak{m}=\sum_{i}^{N}\hbar \omega \left(n_i+\tfrac{1}{2}\right)$.
The total partition function is given by
\begin{align} 
  Z &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_\mathfrak{m}}\\
    &= \sum_{ \left\{n\right\}}^{}\text{e}^{-\beta \sum_{i}^{N}\hbar \omega (n_i+\tfrac{1}{2})}
.\end{align} 
Since they are all non-interacting the partition function factorizes
\begin{align} 
  Z &= Z_1^N\\
    &= \left(\sum_{n_i}^{}\text{e}^{-\beta \hbar \omega \left(n_i+\tfrac{1}{2}\right)}\right)^N\\
    &= \text{e}^{-\hbar \omega \tfrac{N}{2}}\left(\sum_{n_i}^{}\text{e}^{-\beta \hbar \omega n_i}\right)^N\\
    &= \text{e}^{-\hbar \omega \tfrac{N}{2}}\left(\sum_{n_i}^{}\left(\text{e}^{-\beta \hbar \omega }\right)^{n_i}\right)^N
.\end{align} 
Now 
\begin{align} 
  \text{e}^{-\beta \hbar \omega }=\tfrac{1}{\text{e}^{\beta \hbar \omega }}<1 \Leftrightarrow \ln(1)=0<\beta \hbar \omega 
\end{align} 
which is always the case.
Then the series converges
\begin{multline} 
    \text{e}^{-\hbar \omega \tfrac{N}{2}}\left(\sum_{n_i}^{}\left(\text{e}^{-\beta \hbar \omega }\right)^{n_i}\right)^N \\= \text{e}^{-\hbar \omega \tfrac{N}{2}}\left(\dfrac{1}{1-\text{e}^{-\beta \hbar \omega }}\right)^N
.\end{multline} 
Also
\begin{align} 
  \dfrac{1}{1-\text{e}^{-x}} &= \dfrac{1}{\text{e}^{-x/2}\left(\text{e}^{x/2}-\text{e}^{-x/2}\right)}=\dfrac{\text{e}^{x/2}}{\text{e}^{x/2}-\text{e}^{-x/2}}\\
                             &= \dfrac{\text{e}^{x/2}}{2\sinh x}
.\end{align} 
Thereby
\begin{align} 
  Z &= \text{e}^{-\hbar \omega \tfrac{N}{2}}\left(\dfrac{\text{e}^{\hbar \omega /2}}{\sinh \left(\beta \hbar \omega \right)}\right)^N\\
    &= \sinh \left(\beta \hbar \omega \right)^{-N}=\dfrac{1}{\sinh \left(\beta \hbar \omega \right)^N}
.\end{align} 
%}}}

%{{{ Grand Canonical Ensemble
\subsection{Grand Canonical Ensemble}
The ensemble is specified to have a fixed pressure, volume and temperature resulting in an unknown energy and particle number.
It is simply the canonical ensemble but with varying / unspecified particle number.
The probability that a certain microstate $\mathfrak{m}$ realizes a macrostate whose particle number is $N$ and energy $E_{\mathfrak{m}(N)}$ (the energy or energy microstates depend on the particle number, which is indicated by $\mathfrak{m}(N)$) is then given by
\begin{align} 
  p_\mathfrak{m} &= \dfrac{\text{e}^{-\beta E_{\mathfrak{m}(N)}+\alpha N}}{\sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_{\mathfrak{m}(N)}+\alpha N}}
,\end{align} 
with
\begin{align} 
  \mathcal{Z} &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_{\mathfrak{m}(N)}+\alpha N}\\
              &= \sum_{N}^{}\text{e}^{\alpha N}\sum_{\mathfrak{m}(N)}^{}\text{e}^{-\beta E_{\mathfrak{m}(N)}}\\
              &= \sum_{N}^{}\text{e}^{\alpha N}Z_N
\end{align} 
and $Z_N$ the canonical partition sum for a given particle number $N$.
The sum over all microstates has to be split into two sums $\sum_{\mathfrak{m}}^{}=\sum_{N}^{}\sum_{\mathfrak{m}(N)}^{}$.

The entropie is given by
\begin{align} 
  S &= -k_B\sum_{i}^{}p_i\ln p_i\\
    &= -k_B\sum_{i}^{}p_i\left(-\beta E_i+\alpha N_i-\ln \mathcal{Z}\right)\\
    &= k_B\beta \left\langle E\right\rangle -k_B\alpha \left\langle N\right\rangle +k_B\ln\mathcal{Z}\\
    &= \dfrac{U}{T}+\dfrac{\mu }{T}N+k_B\ln\mathcal{Z} \label{eq:entropy_grand_part_sum}
.\end{align} 
To calculate thermodynamic properties the suitable potential is the grand canonical potential
\begin{align} 
  \Phi = U-TS-\mu N\Leftrightarrow S=\dfrac{U}{T}-\dfrac{\mu }{T}N-\dfrac{\Phi }{T}
.\end{align} 
By using (\ref{eq:entropy_grand_part_sum})
\begin{align} 
  \boxed{\Phi =-k_BT\ln\mathcal{Z}}
\end{align} 
and with
\begin{align} 
  \td \Phi =-S\td T-p\td V-N\td \mu 
\end{align} 
thermodynamic properties can be calculated from the microscopic partition sum.

%}}}

%}}}

%{{{ Classical Ideal Gas
\section{Classical Ideal Gas}
%{{{ Equipartition Theorem
\subsection{Equipartition Theorem}
The equipartition theorem states that for a system in thermal equilibrium any quadratic occurrence of a dregree of freedom in the internal energy contributes $\tfrac{1}{2}k_BT$ in terms of thermal energy.
Thus an ideal gas in three dimensions consisting of monoatomic constituents, which only have translational degrees of freedom, has an internal energy of
\begin{align} 
  U &= \sum_{i}^{N}\dfrac{1}{2}m_i|\dot{\vv{r_i}}|^2=\sum_{i}^{N}\dfrac{3}{2}k_BT=\dfrac{3}{2}Nk_BT
.\end{align} 
In general if a system of $N$ constituents has $f$ degrees of freedom, which appear quadratically in the internal energy, then the energy can be written as
\begin{align} 
  \Aboxed{U &= \dfrac{f}{2}Nk_BT}
.\end{align} 
%}}}

%{{{ The Ideal Gas Law
\subsection{The Ideal Gas Law}
The classical ideal gas consists of $N$ non-interacting particles confined to a volume $V$ and with fixed energy $E$.
In the microcanonical ensemble treatment the number of microstates will then be proportional to the product of each particles available space (which will be $V$ since the particles are non-interacting)
\begin{align} 
  \Omega \propto V^N
.\end{align} 
The entropy is then given by
\begin{align}
  S &= k_B\ln \Omega =k_BN\ln V
.\end{align} 
One can now calculate thermodynamic properties by using
\begin{align} 
  \diffp[]{S}{V} &= \dfrac{P}{T} = \dfrac{k_BN}{V}\\
  \Leftrightarrow \Aboxed{PV &= Nk_BT}
.\end{align} 
This is the ideal gas law.

Consider an adiabatic process in which $\delta Q=0$, thus $\td U=-P\td V$.
The internal energy is given by the equipartition theorem as
\begin{align} 
  U &= \dfrac{f}{2}PV = \dfrac{f}{2}Nk_BT
.\end{align} 
This is also sometimes written as
\begin{align} 
  P &= \dfrac{2}{f}\dfrac{U}{V}
,\end{align} 
the pressure is equal to two over the degrees of freedom of the energy density.
Now the change of $U$ depends on the change of $P$ and $V$ or $T$
\begin{align} 
  \td U &= \dfrac{f}{2}\left(V\td P + P\td V\right) = \dfrac{f}{2}Nk_B\td T
.\end{align} 
Due to the first law of thermodynamics this is also equal to
\begin{align} 
  \Leftrightarrow \dfrac{f}{2}(V\td P + P\td V) &= -P\td V \\
  \Leftrightarrow P\left(\dfrac{f}{2}+1\right)\td V &= -\dfrac{f}{2}V\td P\\
  \Leftrightarrow \underbrace{\dfrac{\tfrac{f}{2}+1}{\tfrac{f}{2}}}_{=:\gamma }\dfrac{\td V}{V} &= -\dfrac{\td P}{P}\\
  \Leftrightarrow -\gamma \ln\left(\dfrac{V_2}{V_1}\right) &= \ln\left(\dfrac{P_2}{P_1}\right)\\
  \Leftrightarrow \left(\dfrac{V_1}{V_2}\right)^{\gamma } &= \dfrac{P_2}{P_1}\\
  \Leftrightarrow P_1V_1^\gamma  &= P_2V_2^\gamma =\text{const}
.\end{align} 
also $\tfrac{2}{f}UV^{\gamma -1}=Nk_BTV^{\gamma -1}=\text{const}$, which is equivalent to (since $k_B$ and $N$ are constant) $TV^{\gamma -1}=\text{const}$.
Thus for any reversible adiabatic process
\begin{align} 
  PV^\gamma  &= \text{const} & TV^{\gamma -1} &= \text{const}
.\end{align} 
The heat capacity at constant volume is given by
\begin{align} 
  c_V &= \diffp[]{U}{T} = \dfrac{f}{2}Nk_B
.\end{align} 
The heat capacity at constant pressure is best calculated when the fixed variables of state contains the pressure and the volume is variable
\begin{align} 
  c_P &= \diffp[]{(U+PV)}{T} = \left(\dfrac{f}{2}+1\right)Nk_B
.\end{align} 
Therefore
\begin{align} 
  \Aboxed{c_P&>c_V} & \dfrac{c_P}{c_V}&=\dfrac{\tfrac{f}{2}+1}{\tfrac{f}{2}}=\gamma 
.\end{align} 
This hold generally.
For a constant volume the added heat can only be transferred to the temperature of the system, but for constant pressure the heat can also be used to expand the gas.
%}}}
%}}}

%{{{ Density of States
\section{Density of States}
Consider an ensemble of a system and its time evolution in phase space as a swarm of points in a certain allowed region (consistent with the macrostate) of phase space.
Then there exists a function $\rho \left(q,p,t\right)$ that gives the density of the representative points in phase space and $\rho \left(q,p,t\right)\td q ^{3N}\td p ^{3N}$ will give the total amount of points in a specific area.
This function is called the density of states.
The ensemble average of a given physical quantity is thus
\begin{align} 
  \left\langle f \right\rangle  &= \dfrac{\int_{}^{}\td q ^{3N}\td p ^{3N}f\left(q,p\right)\rho \left(q,p,t\right)}{\int_{}^{}\td q ^{3N}\td p ^{3N}\rho (q,p,t)}
.\end{align} 
For $\rho =\rho (q,p)$ i.e.\ $\partial_t \rho =0$ a system is said to be stationary and thus $\partial_t\left\langle f \right\rangle =0$.
Such a system is in equlibrium.

The density of states in $d$ dimensions and for $N$ constituents (e.g.\ particles) is given by
\begin{align} 
  \rho _d &= \int_{}^{}\td \Gamma \delta \left(E-E(q,p)\right)\\
        &= \int_{}^{}\td q ^{d\cdot N}\td p ^{d\cdot N}\delta (E-E(q,p))
.\end{align} 
It is the phase space integral times the delta distribution which peaks at the allowed energies of the system.
If the energy does not depend on position, then $\int_{}^{}\td q ^{d\cdot N}=V_d^N$ will be just the volume.
In the quantum mechanical case the momentum integral will be $\int_{}^{}\tfrac{\td p ^{d\cdot N}}{(2\pi \hbar )^d}$.

%{{{ Examples
\subsection{Examples}
Consider the classical and relativistic dispersion relations
\begin{align} 
  E_c &= \dfrac{p^2}{2m} & E_r &= cp\\
  \diff[]{E_c}{p} &= \dfrac{p}{m} & \diffp[]{E_r}{p} &= c\\
  \td E_c \tfrac{m}{p} &= \td p & \td E_r \tfrac{1}{c} &= \td p\\
  \td E_c \tfrac{m}{\,\sqrt[]{2mE_r}} &= \td p
.\end{align} 
The density of states in different dimensions is given by
\begin{align} 
  \rho _1 &= \int_{}^{}\td q\tfrac{\td p}{2\pi \hbar } \begin{cases}
    \delta (E-E_c)\\
    \delta (E-E_r)
  \end{cases}\\
          &= \dfrac{V_1}{2\pi \hbar }\int_{}^{}\td E \begin{cases}
            \tfrac{m}{\,\sqrt[]{2mE}}\delta (E-E_c)\\
            \tfrac{1}{c}\delta (E-E_r)
          \end{cases}\\
          &= \dfrac{V_1}{2\pi \hbar }\begin{cases}
            \tfrac{m}{\,\sqrt[]{2mE_r}}\\
            \tfrac{1}{c}
          \end{cases}\\
          &= \begin{cases}
            \tfrac{V_1}{\pi \hbar }\,\sqrt[]{\tfrac{2m}{E_r}}\\
            \tfrac{V_1}{2\pi \hbar c}
          \end{cases}
.\end{align} 
\begin{align} 
  \rho _2 &= \int_{}^{}\td q^2\tfrac{\td p^2}{(2\pi \hbar )^2}\begin{cases}
    \delta (E-E_c)\\
    \delta (E-E_r)
  \end{cases}
.\end{align} 
In the higher dimensional case in general one can rewrite
\begin{align} 
  \td p^d &= \td V_d = S_d\td p = S_d^\id p^{d-1}\td p 
\end{align} 
where $V_d$ is the $d$-dimensional volume and $S_d^\id$ is the $d$-dimensional unit sphere.
\begin{align} 
  \td p &= \td V_1 = S_1\td p = S_1^\id p^{1-1}\td p = 2\td p
\end{align} 

%}}}
%}}}
%}}}

%{{{ Occupation Number
\section{Occupation Number}
%{{{ The Gibbs Paradox
\subsection{The Gibbs Paradox}
Consider an ideal monoatomic gas in the canonical treatment.
The partition function is given by
\begin{align} 
  Z &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_\mathfrak{m}}
,\end{align} 
with $E_\mathfrak{m}=\sum_{i}^{N}\tfrac{1}{2m}\left(\tfrac{2\pi \hbar }{L}\right)^2|\vv{n}_i|^2=:\sum_{i}^{N}\Delta |\vv{n}_i|^2$ with $\vv{n}_i=(n_{i_x}, n_{i_y}, n_{i_z})$ the vector of quantum numbers of the $i^\text{th}$ particle
\begin{align} 
  Z &= \sum_{ \left\{n\right\}}^{}\text{e}^{-\beta \sum_{i}^{3N}\Delta n_i^2}\\
    &= \left(\sum_{n_i}^{}\text{e}^{-\beta \Delta n_i^2}\right)^{3N}
.\end{align} 
In the thermodynamic limit the energy levels will be close together such that the sum can be replaced with an integral $(n\equiv n_i)$ 
\begin{align} 
  Z_1 &= \int_{-\infty}^{\infty}\td n \text{e}^{-\beta \Delta n^2} = \,\sqrt[]{\dfrac{\pi }{\beta \Delta }}
.\end{align} 
Also note
\begin{align} 
  \,\sqrt[]{\dfrac{\pi }{\beta \Delta }} &= \,\sqrt[]{\dfrac{k_BT2m\pi }{(2\pi \hbar )^2}}L = \,\sqrt[]{\dfrac{k_BTm}{2\pi \hbar ^2}}L =: \dfrac{L}{\lambda _T}\\
  \lambda _T &:= \,\sqrt[]{\dfrac{2\pi \hbar ^2}{k_BTm}} = \dfrac{h}{\,\sqrt[]{2\pi k_BTm}}
,\end{align} 
the thermal wavelength (i.e.\ the wavelength of a particle due to its thermal energy).
The whole partition sum is thus
\begin{align} 
  Z &= \left(\dfrac{L}{\lambda _T}\right)^{3N}=\left(\dfrac{V}{\lambda _T^3}\right)^N=\left(\dfrac{\pi }{\beta \Delta }\right)^{\tfrac{3}{2}N}
.\end{align} 
More generally $3N\rightarrow dN$ for $d$ dimensions.
The free energy is
\begin{align} 
  F &= -k_BT\ln Z\\
    &= -Nk_BT\left(\ln V - 3\ln \lambda _T\right)\\
    &= -Nk_BT\dfrac{3}{2}\ln \left(\pi \dfrac{k_BT}{\Delta }\right)
.\end{align} 
The entropy
\begin{align} 
  S &= -\diffp[]{F}{T} = Nk_B\dfrac{3}{2}\ln\left(\pi \dfrac{k_BT}{\Delta }\right)+\dfrac{3}{2}Nk_B
,\end{align} 
with $\Delta =\tfrac{1}{2m}\left(\tfrac{2\pi \hbar }{L}\right)^2=\tfrac{2\pi ^2\hbar ^2}{m}\tfrac{1}{L^2}$
\begin{align} 
  S &= Nk_B\dfrac{3}{2}\ln \left(L^2\dfrac{k_BTm}{2\pi \hbar ^2}\right)+\dfrac{3}{2}Nk_B\\
    &= Nk_B\ln V + \dfrac{3}{2}Nk_B\left(\ln\left(\dfrac{k_BTm}{2\pi \hbar ^2}\right) + 1\right)
\end{align} 
The internal energy
\begin{align} 
  U &= F+TS = \dfrac{3}{2}Nk_BT & c_V &= \diffp[]{U}{T}=\dfrac{3}{2}Nk_B
,\end{align} 
as expected from the equipartition theorem for the monoatomic ideal gas (the calculation of the partition sum was therefore right).

Now consider two identical ideal monoatomic gases confined to volumes $V_1$ and $V_2$ with particle number $N_1$ and $N_2$ in thermal equlibrium $T_1=T_2=T$ and of same mass.
The two containers of the gases will then be brought in contact and a sliding wall between them is lifted such that they mix.
Their respective entropies are given by
\begin{align} 
  S_i &= N_ik_B\ln V_i + \dfrac{3}{2}N_ik_B\left(\ln\left(\dfrac{k_BTm}{2\pi \hbar ^2}\right) + 1\right)
.\end{align} 
The entropy after the two gases mixed will then be the sum of the two entropies but now dependent on and $V=V_1+V_2$ because both gases are allowed to the same volume
\footnote{Note the dependence on $V$ for both entropies and how it is left out in the sum.
Therefore summing over $i$ will result in the same $V$ for both entropies.
In the case before mixing the sum is still over $V_i$ since they are confined to their respective region in the box.}
\begin{align} 
    S_m &= \sum_{i}^{2}N_ik_B\ln V + \dfrac{3}{2}N_ik_B\left(\ln\left(\dfrac{k_BTm}{2\pi \hbar ^2}\right) + 1\right)
.\end{align} 
Since the gases are identical one can slide the wall back in and should arrive at the situation from the start (because one cannot distinguish the particles in the gas).
There should then be no mixing entropy, however
\begin{align} 
  \Delta S &= S_m-\sum_{i}^{2}S_i\\
           &=
\end{align} 

%}}}
%{{{ Derivation
\subsection{Derivation}
It is not always wise or even correct to take identical and non-localized constituents of a system (e.g.\ particles in a gas) as distinguishable and let the change of their respective positions count as a separate microstate (see for example the Gibbs paradox !TODO!).
To calculate the correct value for the probability distribution and the partition sum of indistinguishable particles one has to use the theory of occupation numbers.

Consider a quantum gas made up of such indistinguishable particles on different energy levels.
These particles will either be Fermions or Bosons.
The levels / energies are labeled as $\varepsilon _1,\varepsilon _2,\varepsilon _3,\hdots $.
The information the occupation number $n_1,n_2,n_3,\hdots $ tells is "How many particles $n_i$ are on the energy level $\varepsilon _i$?".
As a result, when two particles swap their energies (i.e.\ particle $A$ goes from $\varepsilon _A$ to $\varepsilon _B$ and particle $B$ goes from $\varepsilon _B$ to $\varepsilon _A$) it will not be counted as a separate state because the only indicator for a distinguished state is the occupation number, which will not have changed.
For Fermions $n_i  \in \left[0,1\right]$ and for Bosons $n_i  \in [0,\infty)$ due to the Pauli principle.

The occupation number is useful in the grand canonical ensemble since the particle number is not fixed.
This makes the calculation easier to do because every particle does not need to be counted individually but only the number of particles per energy level.
\footnote{Occupation numbers can also be used in a canonical system, but the constraint $N=\text{const}$ will make the sum over the occupation numbers dependend on each other such that there is no overcounting.
This makes executing the sum difficult (but not impossible).}
The partition sum in general is
\begin{align} 
  \mathcal{Z} &= \sum_{\mathfrak{m}}^{}\text{e}^{-\beta E_{\mathfrak{m}(N)}+\alpha N}
.\end{align} 
Now the energy is not the particle energy sum but rather
\begin{align} 
  E_\mathfrak{m}(N)=\sum_{i}^{}\varepsilon _i n_i
\end{align} 
the sum over all energy levels (denoted by $\varepsilon _i$) and their respective occupation number (denoted by $n_i$) which is the amount of particles on that energy level.
The total number of particles within the system is therefore
\begin{align} 
  N=\sum_{i}^{}n_i
.\end{align} 
The sum over all microstates is not the sum over all states of each individual particle (as e.g.\ a sum over all quantum numbers a single particle can have, or all available angles to a single dipole) but rather the sum over all values of the occupation numbers.
\begin{align} 
  \mathcal{Z} &= \sum_{ \left\{n\right\}}^{}\text{e}^{\sum_{i}^{}(-\beta \varepsilon _i n_i+\alpha n_i)}\\
              &= \sum_{ \left\{n\right\}}^{}\text{e}^{\sum_{i}^{}(\alpha -\beta \varepsilon _i)n_i}\\
              &= \sum_{ \left\{n\right\}}^{}\prod_{i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}\\
              &= \sum_{n_1}^{}\sum_{n_2}^{}\hdots \text{e}^{(\alpha -\beta \varepsilon _1)n_1}\text{e}^{(\alpha -\beta \varepsilon _2)n_2}\hdots \\
              &= \sum_{n_1}^{}\text{e}^{(\alpha -\beta \varepsilon _1)n_1}\sum_{n_2}^{}\text{e}^{(\alpha -\beta \varepsilon _2)n_2}\hdots \\
              &= \prod_{i}^{}\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}\\
              &= \prod_{i}^{}\mathcal{Z}_i
,\end{align} 
with $\mathcal{Z}_i$ the $i^{\text{th}}$ energy level partition sum.
One can now ask the question "What is the mean occupation number in a system or Fermions/Bosons?".
%}}}

%{{{ Fermi-Dirac Statistics
\subsection{Fermi-Dirac Statistics}
For Fermions $n_i  \in \left[0,1\right]$ thus
\begin{align} 
  \mathcal{Z}=\prod_{i}^{}\left(1+\text{e}^{\alpha -\beta \varepsilon _i}\right)
.\end{align} 
The average occupation number for a single energy level $i$ is calculated via !TODO! DISCREPANCY BETWEEN AVERAGE VALUE CALCULATION SUMMATION INDICES
\begin{align} 
  \left\langle n_i\right\rangle  &= \sum_{ \left\{n\right\}}^{}p_i n_i\\
                                 &= \sum_{ \left\{n\right\}}^{}\dfrac{1}{\mathcal{Z}}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i\\
                                 &= \sum_{ \left\{n\right\}}^{}\dfrac{\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i}{\sum_{ \left\{n\right\}}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}}
\end{align} 
all sums $\sum_{n_j}^{}\hdots \sum_{n_k}^{}$ that are not about $n_i$ will cancel with the partition function
\begin{align} 
  \left\langle n_i\right\rangle  &= \dfrac{\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i}{\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}}\\
                                 &= \dfrac{0+\text{e}^{\alpha -\beta \varepsilon _i}}{1+\text{e}^{\alpha -\beta \varepsilon _i}}\\
                                 &= \boxed{\dfrac{1}{\text{e}^{-(\alpha -\beta \varepsilon _i)}+1} =: f(\varepsilon _i)}
.\end{align} 
This is the Fermi distribution.
It holds not only for the single energy level $i$ but more generally for the whole system.
%}}}

%{{{ Bose-Einstein Statistics
\subsection{Bose-Einstein Statistics}
For Bosons $n_i  \in [0,\infty)$ thus
\begin{align} 
  \mathcal{Z} &= \prod_{i}^{}\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}
.\end{align} 
Consider
\begin{align} 
  \text{e}^{\alpha -\beta \varepsilon _i}&<1\\
  \Leftrightarrow \alpha -\beta \varepsilon _i&<0\\
  \Leftrightarrow \mu &<\varepsilon _i
\end{align} 
which is always the case for Bosons.
The physical interpretation is that a system of Bosons has no restrictions in which state a new Boson can go and thus will in general increase its energy with the additions $(\mu <0)$ or at most nothing will happen $(\mu =0)$\footnote{This is the case for photons.}.
If this mathematical constraint holds then the sum will converge
\begin{align} 
  \mathcal{Z} &= \prod_{i}^{}\dfrac{1}{1-\text{e}^{\alpha -\beta \varepsilon _i}}=\prod_{i}^{}\mathcal{Z}_i
.\end{align} 

The average occupation number for a single energy level $i$ is calculated via !TODO! SEE ABOVE
\begin{align} 
  \left\langle n_i\right\rangle  &= \sum_{ \left\{n\right\}}^{}p_i n_i\\
                                 &= \sum_{ \left\{n\right\}}^{}\dfrac{1}{\mathcal{Z}}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i\\
                                 &= \sum_{ \left\{n\right\}}^{}\dfrac{\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i}{\sum_{ \left\{n\right\}}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}}
\end{align} 
all sums $\sum_{n_j}^{}\hdots \sum_{n_k}^{}$ that are not about $n_i$ will cancel with the partition function
\begin{align} 
  \left\langle n_i\right\rangle  &= \dfrac{\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}n_i}{\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}}\\
                                 &= \diffp[]{\ln\left(\sum_{n_i}^{}\text{e}^{(\alpha -\beta \varepsilon _i)n_i}\right)}{(\alpha -\beta \varepsilon _i)}\\
                                 &= \diffp[]{\ln \mathcal{Z}_i}{(\alpha -\beta \varepsilon _i)}\\
                                 &= -\diffp[]{\ln \left(1-\text{e}^{\alpha -\beta \varepsilon _i}\right)}{(\alpha -\beta \varepsilon _i)}\\
                                 &= \dfrac{\text{e}^{\alpha -\beta \varepsilon _i}}{1-\text{e}^{\alpha -\beta \varepsilon _i}}\\
                                 &=\boxed{\dfrac{1}{\text{e}^{-(\alpha -\beta \varepsilon _i)}-1}=:b(\varepsilon _i)}
.\end{align} 
This is the Bose distribution.
It holds not only for the single energy level $i$ but more generally for the whole system.

These distributions are quite similar and only differ by a sign; plus for Fermions and minus for Bosons.
If one were to use $\beta =\tfrac{1}{k_BT}$ and $\alpha =\tfrac{\mu }{k_BT}$ and take the minus sign into the parentheses
\begin{align} 
  \Aboxed{\dfrac{1}{\text{e}^{\tfrac{\varepsilon _i-\mu }{k_BT}}\pm 1}} &= \begin{cases}
    f(\varepsilon _i)&\text{for}\,+\\ b(\varepsilon _i)&\text{for}\,-
  \end{cases}
.\end{align} 

%}}}
%}}}

\end{multicols}

%\clearpage
%\listoffigures
%\listoftables
%\bibliographystyle{plain}
%\bibliography{refs}

%}}}

\end{document}
